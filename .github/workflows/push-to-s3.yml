name: Build and Push to S3 Cache

on:
  push:
    paths:
      - "flake.nix"
      - ".github/workflows/push-to-s3.yml"
      - ".github/actions/setup-nix-env/action.yml"

  workflow_dispatch:

jobs:
  verify-write:
    runs-on: self-hosted-nix
    steps:
      - name: Verify S3 Write
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_KEY }}
          S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
          S3_REGION: ${{ secrets.S3_REGION }}
        run: |
          # Use nix shell to ensure awscli is available for the test
          nix shell nixpkgs#awscli2 --command bash -c "
            echo 'Testing S3 writability to $S3_BUCKET...'
            echo 'test-connectivity' > s3_write_test.txt
            aws --endpoint-url https://$S3_ENDPOINT s3 cp s3_write_test.txt s3://$S3_BUCKET/write_test.txt --region $S3_REGION
            aws --endpoint-url https://$S3_ENDPOINT s3 rm s3://$S3_BUCKET/write_test.txt --region $S3_REGION
            rm s3_write_test.txt
            echo 'S3 write check passed.'
          "

  build-and-push:
    needs: verify-write
    strategy:
      matrix:
        system: [x86_64-linux]
    # Use the x86_64 host for all builds (requires remote builders or cross-compilation for aarch64-darwin)
    runs-on: self-hosted-nix
    steps:

      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Nix Shared Environment
        uses: coderbunker/modern-resume-env/.github/actions/setup-nix-env@main
        with:
          s3_bucket: ${{ secrets.S3_BUCKET }}
          s3_endpoint: ${{ secrets.S3_ENDPOINT }}
          github_access_token: ${{ secrets.GITHUB_TOKEN }}


      - name: Build DevShell
        run: |
          # Build the default devShell for the matrix system architecture
          # This ensures all dependencies are in the local store
          nix build .#devShells.${{ matrix.system }}.default --no-link

      - name: Push to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_KEY }}
          S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }} # e.g., s3.bhs.io.cloud.ovh.net
          S3_BUCKET: ${{ secrets.S3_BUCKET }}     # e.g., modern-resume-nix-cache
          S3_REGION: ${{ secrets.S3_REGION }}     # e.g., bhs
          CACHE_SIGNING_KEY: ${{ secrets.CACHE_SIGNING_KEY }}
        run: |
          # 1. Ensure nix-cache-info exists and is public (enables HTTP reading)
          # Nix copy usually handles this, but explicit upload ensures public ACL for HTTP access
          cat <<EOF > nix-cache-info
          StoreDir: /nix/store
          WantMassQuery: 1
          Priority: 30
          EOF
          nix shell nixpkgs#awscli2 --command bash -c "
            aws --endpoint-url https://$S3_ENDPOINT s3 cp nix-cache-info s3://$S3_BUCKET/nix-cache-info \
              --acl public-read \
              --region $S3_REGION
          "
          rm nix-cache-info

          # 2. Write signing key to a temporary file
          echo "$CACHE_SIGNING_KEY" > cache-priv-key.pem

          # 3. Archive and Copy Flake Inputs (Sources) to S3
          # 'nix develop' inside Docker needs the source code of inputs (e.g. nixpkgs) to evaluate the flake.
          # We use 'nix flake archive' to fetch them to the local store and get their paths.
          nix shell nixpkgs#jq nixpkgs#git --command bash -c "
            INPUT_PATHS=\$(nix flake archive --json | jq -r '[.. | .path? | strings] | unique | .[]')
            echo \"Copying flake inputs to S3: \$INPUT_PATHS\"
            nix copy --to \"s3://$S3_BUCKET?endpoint=$S3_ENDPOINT&region=$S3_REGION&secret-key=$(pwd)/cache-priv-key.pem\" \$INPUT_PATHS
          "

          # 4. Copy and sign all derivations referenced by the devShell to the S3 bucket
          nix copy --to "s3://$S3_BUCKET?endpoint=$S3_ENDPOINT&region=$S3_REGION&secret-key=$(pwd)/cache-priv-key.pem" \
            .#devShells.${{ matrix.system }}.default

          rm cache-priv-key.pem
